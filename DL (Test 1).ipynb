{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-05T07:11:42.774653400Z",
     "start_time": "2024-03-05T07:11:42.755662800Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import base64\n",
    "from IPython.display import display, HTML\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Importing necessary libraries and modules\n",
    "import warnings  # Import the 'warnings' module for handling warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore warnings during execution\n",
    "\n",
    "import gc  # Import the 'gc' module for garbage collection\n",
    "import numpy as np  # Import NumPy for numerical operations\n",
    "import pandas as pd  # Import Pandas for data manipulation\n",
    "import itertools  # Import 'itertools' for iterators and looping\n",
    "from collections import Counter  # Import 'Counter' for counting elements\n",
    "import matplotlib.pyplot as plt  # Import Matplotlib for data visualization\n",
    "from sklearn.metrics import (  # Import various metrics from scikit-learn\n",
    "    accuracy_score,  # For calculating accuracy\n",
    "    roc_auc_score,  # For ROC AUC score\n",
    "    confusion_matrix,  # For confusion matrix\n",
    "    classification_report,  # For classification report\n",
    "    f1_score  # For F1 score\n",
    ")\n",
    "\n",
    "# Import custom modules and classes\n",
    "from imblearn.over_sampling import RandomOverSampler # import RandomOverSampler\n",
    "import accelerate # Import the 'accelerate' module\n",
    "import evaluate  # Import the 'evaluate' module\n",
    "from datasets import Dataset, Image, ClassLabel  # Import custom 'Dataset', 'ClassLabel', and 'Image' classes\n",
    "from transformers import (  # Import various modules from the Transformers library\n",
    "    TrainingArguments,  # For training arguments\n",
    "    Trainer,  # For model training\n",
    "    ViTImageProcessor,  # For processing image data with ViT models\n",
    "    ViTForImageClassification,  # ViT model for image classification\n",
    "    DefaultDataCollator  # For collating data in the default way\n",
    ")\n",
    "import torch  # Import PyTorch for deep learning\n",
    "from torch.utils.data import DataLoader  # For creating data loaders\n",
    "from torchvision.transforms import (  # Import image transformation functions\n",
    "    CenterCrop,  # Center crop an image\n",
    "    Compose,  # Compose multiple image transformations\n",
    "    Normalize,  # Normalize image pixel values\n",
    "    RandomRotation,  # Apply random rotation to images\n",
    "    RandomResizedCrop,  # Crop and resize images randomly\n",
    "    RandomHorizontalFlip,  # Apply random horizontal flip\n",
    "    RandomAdjustSharpness,  # Adjust sharpness randomly\n",
    "    Resize,  # Resize images\n",
    "    ToTensor  # Convert images to PyTorch tensors\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "142f37ec3077aa93"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Import the necessary module from the Python Imaging Library (PIL).\n",
    "from PIL import ImageFile\n",
    "\n",
    "# Enable the option to load truncated images.\n",
    "# This setting allows the PIL library to attempt loading images even if they are corrupted or incomplete.\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c9b9b5d555b3834"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# use https://huggingface.co/docs/datasets/image_load for reference\n",
    "\n",
    "# Import necessary libraries\n",
    "image_dict = {}\n",
    "\n",
    "# Define the list of file names\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "# Initialize empty lists to store file names and labels\n",
    "file_names = []\n",
    "labels = []\n",
    "\n",
    "# Iterate through all image files in the specified directory\n",
    "for file in tqdm(sorted((Path('/kaggle/input/hagrid-classification-512p/hagrid-classification-512p/').glob('*/*.*')))):\n",
    "    label = str(file).split('/')[-2]  # Extract the label from the file path\n",
    "    labels.append(label)  # Add the label to the list\n",
    "    file_names.append(str(file))  # Add the file path to the list\n",
    "\n",
    "# Print the total number of file names and labels\n",
    "print(len(file_names), len(labels))\n",
    "\n",
    "# Create a pandas dataframe from the collected file names and labels\n",
    "df = pd.DataFrame.from_dict({\"image\": file_names, \"label\": labels})\n",
    "print(df.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a220d6332ef83d3b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df.head()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c92e598216ab2574"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['label'].unique()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4d7a3029367f30c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# random oversampling of minority class\n",
    "# 'y' contains the target variable (label) we want to predict\n",
    "y = df[['label']]\n",
    "\n",
    "# Drop the 'label' column from the DataFrame 'df' to separate features from the target variable\n",
    "df = df.drop(['label'], axis=1)\n",
    "\n",
    "# Create a RandomOverSampler object with a specified random seed (random_state=83)\n",
    "ros = RandomOverSampler(random_state=83)\n",
    "\n",
    "# Use the RandomOverSampler to resample the dataset by oversampling the minority class\n",
    "# 'df' contains the feature data, and 'y_resampled' will contain the resampled target variable\n",
    "df, y_resampled = ros.fit_resample(df, y)\n",
    "\n",
    "# Delete the original 'y' variable to save memory as it's no longer needed\n",
    "del y\n",
    "\n",
    "# Add the resampled target variable 'y_resampled' as a new 'label' column in the DataFrame 'df'\n",
    "df['label'] = y_resampled\n",
    "\n",
    "# Delete the 'y_resampled' variable to save memory as it's no longer needed\n",
    "del y_resampled\n",
    "\n",
    "# Perform garbage collection to free up memory used by discarded variables\n",
    "gc.collect()\n",
    "\n",
    "print(df.shape)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9def5fa2331aff8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a dataset from a Pandas DataFrame.\n",
    "dataset = Dataset.from_pandas(df).cast_column(\"image\", Image())"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2cfabf19e1fceca3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Display the first image in the dataset\n",
    "dataset[0][\"image\"]"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a8d10f1e01a91819"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extracting a subset of elements from the 'labels' list using slicing.\n",
    "# The slicing syntax [:5] selects elements from the beginning up to (but not including) the 5th element.\n",
    "# This will give us the first 5 elements of the 'labels' list.\n",
    "# The result will be a new list containing these elements.\n",
    "labels_subset = labels[:5]\n",
    "\n",
    "# Printing the subset of labels to inspect the content.\n",
    "print(labels_subset)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c8622e193cd7019d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a list of unique labels by converting 'labels' to a set and then back to a list\n",
    "labels_list = sorted(list(set(labels)))\n",
    "\n",
    "# Initialize empty dictionaries to map labels to IDs and vice versa\n",
    "label2id, id2label = dict(), dict()\n",
    "\n",
    "# Iterate over the unique labels and assign each label an ID, and vice versa\n",
    "for i, label in enumerate(labels_list):\n",
    "    label2id[label] = i  # Map the label to its corresponding ID\n",
    "    id2label[i] = label  # Map the ID to its corresponding label\n",
    "\n",
    "# Print the resulting dictionaries for reference\n",
    "print(\"Mapping of IDs to Labels:\", id2label, '\\n')\n",
    "print(\"Mapping of Labels to IDs:\", label2id)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d427d8be4b5a623e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Creating classlabels to match labels to IDs\n",
    "ClassLabels = ClassLabel(num_classes=len(labels_list), names=labels_list)\n",
    "\n",
    "# Mapping labels to IDs\n",
    "def map_label2id(example):\n",
    "    example['label'] = ClassLabels.str2int(example['label'])\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(map_label2id, batched=True)\n",
    "\n",
    "# Casting label column to ClassLabel Object\n",
    "dataset = dataset.cast_column('label', ClassLabels)\n",
    "\n",
    "# Splitting the dataset into training and testing sets using an 90-10 split ratio.\n",
    "dataset = dataset.train_test_split(test_size=0.1, shuffle=True, stratify_by_column=\"label\")\n",
    "\n",
    "# Extracting the training data from the split dataset.\n",
    "train_data = dataset['train']\n",
    "\n",
    "# Extracting the testing data from the split dataset.\n",
    "test_data = dataset['test']"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ba1585db63fd36e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the pre-trained ViT model string\n",
    "model_str = 'dima806/hand_gestures_image_detection' #'google/vit-base-patch16-224-in21k'\n",
    "\n",
    "# Create a processor for ViT model input from the pre-trained model\n",
    "processor = ViTImageProcessor.from_pretrained(model_str)\n",
    "\n",
    "# Retrieve the image mean and standard deviation used for normalization\n",
    "image_mean, image_std = processor.image_mean, processor.image_std\n",
    "\n",
    "# Get the size (height) of the ViT model's input images\n",
    "size = processor.size[\"height\"]\n",
    "print(\"Size: \", size)\n",
    "\n",
    "# Define a normalization transformation for the input images\n",
    "normalize = Normalize(mean=image_mean, std=image_std)\n",
    "\n",
    "# Define a set of transformations for training data\n",
    "_train_transforms = Compose(\n",
    "    [\n",
    "        Resize((size, size)),             # Resize images to the ViT model's input size\n",
    "        RandomRotation(90),               # Apply random rotation\n",
    "        RandomAdjustSharpness(2),         # Adjust sharpness randomly\n",
    "        RandomHorizontalFlip(0.5),        # Random horizontal flip\n",
    "        ToTensor(),                       # Convert images to tensors\n",
    "        normalize                         # Normalize images using mean and std\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define a set of transformations for validation data\n",
    "_val_transforms = Compose(\n",
    "    [\n",
    "        Resize((size, size)),             # Resize images to the ViT model's input size\n",
    "        ToTensor(),                       # Convert images to tensors\n",
    "        normalize                         # Normalize images using mean and std\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define a function to apply training transformations to a batch of examples\n",
    "def train_transforms(examples):\n",
    "    examples['pixel_values'] = [_train_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
    "    return examples\n",
    "\n",
    "# Define a function to apply validation transformations to a batch of examples\n",
    "def val_transforms(examples):\n",
    "    examples['pixel_values'] = [_val_transforms(image.convert(\"RGB\")) for image in examples['image']]\n",
    "    return examples"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d5f737581ddd975b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set the transforms for the training data\n",
    "train_data.set_transform(train_transforms)\n",
    "\n",
    "# Set the transforms for the test/validation data\n",
    "test_data.set_transform(val_transforms)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bdd2c4c73968c14a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define a collate function that prepares batched data for model training.\n",
    "def collate_fn(examples):\n",
    "    # Stack the pixel values from individual examples into a single tensor.\n",
    "    pixel_values = torch.stack([example[\"pixel_values\"] for example in examples])\n",
    "    \n",
    "    # Convert the label strings in examples to corresponding numeric IDs using label2id dictionary.\n",
    "    labels = torch.tensor([example['label'] for example in examples])\n",
    "    \n",
    "    # Return a dictionary containing the batched pixel values and labels.\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3019137f489851a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a ViTForImageClassification model from a pretrained checkpoint with a specified number of output labels.\n",
    "model = ViTForImageClassification.from_pretrained(model_str, num_labels=len(labels_list))\n",
    "\n",
    "# Configure the mapping of class labels to their corresponding indices for later reference.\n",
    "model.config.id2label = id2label\n",
    "model.config.label2id = label2id\n",
    "\n",
    "# Calculate and print the number of trainable parameters in millions for the model.\n",
    "print(model.num_parameters(only_trainable=True) / 1e6)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "98997909fe98534d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Load the accuracy metric from a module named 'evaluate'\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Define a function 'compute_metrics' to calculate evaluation metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    # Extract model predictions from the evaluation prediction object\n",
    "    predictions = eval_pred.predictions\n",
    "    \n",
    "    # Extract true labels from the evaluation prediction object\n",
    "    label_ids = eval_pred.label_ids\n",
    "    \n",
    "    # Calculate accuracy using the loaded accuracy metric\n",
    "    # Convert model predictions to class labels by selecting the class with the highest probability (argmax)\n",
    "    predicted_labels = predictions.argmax(axis=1)\n",
    "    \n",
    "    # Calculate accuracy score by comparing predicted labels to true labels\n",
    "    acc_score = accuracy.compute(predictions=predicted_labels, references=label_ids)['accuracy']\n",
    "    \n",
    "    # Return the computed accuracy as a dictionary with the key \"accuracy\"\n",
    "    return {\n",
    "        \"accuracy\": acc_score\n",
    "    }"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b71bb6c23aaf460c"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the name of the evaluation metric to be used during training and evaluation.\n",
    "metric_name = \"accuracy\" \n",
    "\n",
    "# Define the name of the model, which will be used to create a directory for saving model checkpoints and outputs.\n",
    "model_name = \"hand_gestures_image_detection\"\n",
    "\n",
    "# Define the number of training epochs for the model.\n",
    "num_train_epochs = 1\n",
    "\n",
    "# Create an instance of TrainingArguments to configure training settings.\n",
    "args = TrainingArguments(\n",
    "    # Specify the directory where model checkpoints and outputs will be saved.\n",
    "    output_dir=model_name,\n",
    "    \n",
    "    # Specify the directory where training logs will be stored.\n",
    "    logging_dir='./logs',\n",
    "    \n",
    "    # Define the evaluation strategy, which is performed at the end of each epoch.\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    \n",
    "    # Set the learning rate for the optimizer.\n",
    "    learning_rate=4e-7,\n",
    "    \n",
    "    # Define the batch size for training on each device.\n",
    "    per_device_train_batch_size=64,\n",
    "    \n",
    "    # Define the batch size for evaluation on each device.\n",
    "    per_device_eval_batch_size=32,\n",
    "    \n",
    "    # Specify the total number of training epochs.\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    \n",
    "    # Apply weight decay to prevent overfitting.\n",
    "    weight_decay=0.02,\n",
    "    \n",
    "    # Set the number of warm-up steps for the learning rate scheduler.\n",
    "    warmup_steps=50,\n",
    "    \n",
    "    # Disable the removal of unused columns from the dataset.\n",
    "    remove_unused_columns=False,\n",
    "    \n",
    "    # Define the strategy for saving model checkpoints (per epoch in this case).\n",
    "    save_strategy='epoch',\n",
    "    \n",
    "    # Load the best model at the end of training.\n",
    "    load_best_model_at_end=True,\n",
    "    \n",
    "    # Limit the total number of saved checkpoints to save space.\n",
    "    save_total_limit=1,\n",
    "    \n",
    "    # Specify that training progress should not be reported\n",
    "    report_to=\"none\"  # log to none\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6007565969a35c62"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create a Trainer instance for fine-tuning a language model.\n",
    "\n",
    "# - `model`: The pre-trained language model to be fine-tuned.\n",
    "# - `args`: Configuration settings and hyperparameters for training.\n",
    "# - `train_dataset`: The dataset used for training the model.\n",
    "# - `eval_dataset`: The dataset used for evaluating the model during training.\n",
    "# - `data_collator`: A function that defines how data batches are collated and processed.\n",
    "# - `compute_metrics`: A function for computing custom evaluation metrics.\n",
    "# - `tokenizer`: The tokenizer used for processing text data.\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=processor,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1b86646293720b72"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ac3a75cb76d18ef"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.train()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "23a1dd33af9aada0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "32015f2f3b63e49a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "outputs = trainer.predict(test_data)\n",
    "print(outputs.metrics)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "559cb81f5dcd0275"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Extract the true labels from the model outputs\n",
    "y_true = outputs.label_ids\n",
    "\n",
    "# Predict the labels by selecting the class with the highest probability\n",
    "y_pred = outputs.predictions.argmax(1)\n",
    "\n",
    "# Define a function to plot a confusion matrix\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion Matrix', cmap=plt.cm.Blues, figsize=(10, 8)):\n",
    "    \"\"\"\n",
    "    This function plots a confusion matrix.\n",
    "\n",
    "    Parameters:\n",
    "        cm (array-like): Confusion matrix as returned by sklearn.metrics.confusion_matrix.\n",
    "        classes (list): List of class names, e.g., ['Class 0', 'Class 1'].\n",
    "        title (str): Title for the plot.\n",
    "        cmap (matplotlib colormap): Colormap for the plot.\n",
    "    \"\"\"\n",
    "    # Create a figure with a specified size\n",
    "    plt.figure(figsize=figsize)\n",
    "    \n",
    "    # Display the confusion matrix as an image with a colormap\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    \n",
    "    # Define tick marks and labels for the classes on the axes\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.0f'\n",
    "    # Add text annotations to the plot indicating the values in the cells\n",
    "    thresh = cm.max() / 2.0\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    # Label the axes\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "    # Ensure the plot layout is tight\n",
    "    plt.tight_layout()\n",
    "    # Display the plot\n",
    "    plt.show()\n",
    "\n",
    "# Calculate accuracy and F1 score\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average='macro')\n",
    "\n",
    "# Display accuracy and F1 score\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Get the confusion matrix if there are a small number of labels\n",
    "if len(labels_list) <= 250:\n",
    "    # Compute the confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    # Plot the confusion matrix using the defined function\n",
    "    plot_confusion_matrix(cm, labels_list, figsize=(12, 10))\n",
    "    \n",
    "# Finally, display classification report\n",
    "print()\n",
    "print(\"Classification report:\")\n",
    "print()\n",
    "print(classification_report(y_true, y_pred, target_names=labels_list, digits=4))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8561d9077c8309f5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# from transformers import pipeline\n",
    "# pipe = pipeline('image-classification', model=model_name, device=0)\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"image-classification\", model=\"dima806/hand_gestures_image_detection\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c83c787dc36a780e"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.17193257808685303},\n {'label': 'call', 'score': 0.0861579105257988},\n {'label': 'two_up_inverted', 'score': 0.07222571969032288},\n {'label': 'dislike', 'score': 0.07141251862049103},\n {'label': 'one', 'score': 0.05971246585249901}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.20112767815589905},\n {'label': 'call', 'score': 0.11463000625371933},\n {'label': 'two_up_inverted', 'score': 0.07788241654634476},\n {'label': 'dislike', 'score': 0.0725891962647438},\n {'label': 'mute', 'score': 0.05355085805058479}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.21874919533729553},\n {'label': 'call', 'score': 0.10415852069854736},\n {'label': 'two_up_inverted', 'score': 0.08136343210935593},\n {'label': 'dislike', 'score': 0.06972312182188034},\n {'label': 'peace_inverted', 'score': 0.05138366296887398}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.1891101598739624},\n {'label': 'call', 'score': 0.10126661509275436},\n {'label': 'two_up_inverted', 'score': 0.0856684148311615},\n {'label': 'dislike', 'score': 0.06844570487737656},\n {'label': 'peace_inverted', 'score': 0.05453087389469147}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.20137380063533783},\n {'label': 'call', 'score': 0.10659080743789673},\n {'label': 'two_up_inverted', 'score': 0.0821925699710846},\n {'label': 'dislike', 'score': 0.06805872172117233},\n {'label': 'peace_inverted', 'score': 0.054362110793590546}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.19446919858455658},\n {'label': 'call', 'score': 0.09588877111673355},\n {'label': 'two_up_inverted', 'score': 0.0723797008395195},\n {'label': 'dislike', 'score': 0.06859717518091202},\n {'label': 'one', 'score': 0.05565262213349342}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.19233176112174988},\n {'label': 'call', 'score': 0.1015869677066803},\n {'label': 'two_up_inverted', 'score': 0.07444114983081818},\n {'label': 'dislike', 'score': 0.06924770772457123},\n {'label': 'peace_inverted', 'score': 0.056198615580797195}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.19230586290359497},\n {'label': 'call', 'score': 0.09925141930580139},\n {'label': 'two_up_inverted', 'score': 0.07286836951971054},\n {'label': 'dislike', 'score': 0.06887853890657425},\n {'label': 'one', 'score': 0.05621149018406868}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.19449396431446075},\n {'label': 'call', 'score': 0.10694875568151474},\n {'label': 'two_up_inverted', 'score': 0.07227522134780884},\n {'label': 'dislike', 'score': 0.06789395213127136},\n {'label': 'peace_inverted', 'score': 0.053708381950855255}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.20051518082618713},\n {'label': 'call', 'score': 0.12090019136667252},\n {'label': 'two_up_inverted', 'score': 0.06863518059253693},\n {'label': 'dislike', 'score': 0.06564415991306305},\n {'label': 'mute', 'score': 0.052877187728881836}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.19632093608379364},\n {'label': 'call', 'score': 0.10741684585809708},\n {'label': 'two_up_inverted', 'score': 0.07109998166561127},\n {'label': 'dislike', 'score': 0.06699977815151215},\n {'label': 'one', 'score': 0.054417163133621216}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.21085762977600098},\n {'label': 'call', 'score': 0.10807725042104721},\n {'label': 'two_up_inverted', 'score': 0.07020342350006104},\n {'label': 'dislike', 'score': 0.0672285407781601},\n {'label': 'peace_inverted', 'score': 0.05309922248125076}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.19974634051322937},\n {'label': 'call', 'score': 0.1082320362329483},\n {'label': 'two_up_inverted', 'score': 0.07024939358234406},\n {'label': 'dislike', 'score': 0.06982922554016113},\n {'label': 'peace_inverted', 'score': 0.05350474268198013}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.18947917222976685},\n {'label': 'call', 'score': 0.10434672981500626},\n {'label': 'two_up_inverted', 'score': 0.0685318186879158},\n {'label': 'dislike', 'score': 0.06577718257904053},\n {'label': 'one', 'score': 0.05752115696668625}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.18624569475650787},\n {'label': 'call', 'score': 0.11422554403543472},\n {'label': 'dislike', 'score': 0.07548738270998001},\n {'label': 'two_up_inverted', 'score': 0.07128503173589706},\n {'label': 'one', 'score': 0.053012531250715256}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'three', 'score': 0.2493734359741211},\n {'label': 'peace', 'score': 0.11569004505872726},\n {'label': 'four', 'score': 0.10440793633460999},\n {'label': 'ok', 'score': 0.07714120298624039},\n {'label': 'three2', 'score': 0.06732301414012909}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.19001559913158417},\n {'label': 'call', 'score': 0.10472498089075089},\n {'label': 'dislike', 'score': 0.07119865715503693},\n {'label': 'two_up_inverted', 'score': 0.06630591303110123},\n {'label': 'one', 'score': 0.05911426618695259}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.1889081597328186},\n {'label': 'call', 'score': 0.11063595861196518},\n {'label': 'dislike', 'score': 0.06817073374986649},\n {'label': 'two_up_inverted', 'score': 0.06646805256605148},\n {'label': 'one', 'score': 0.0568791888654232}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.1844007819890976},\n {'label': 'call', 'score': 0.10746831446886063},\n {'label': 'two_up_inverted', 'score': 0.06875630468130112},\n {'label': 'dislike', 'score': 0.06601448357105255},\n {'label': 'one', 'score': 0.05660797655582428}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.18793649971485138},\n {'label': 'call', 'score': 0.10566120594739914},\n {'label': 'two_up_inverted', 'score': 0.06729278713464737},\n {'label': 'dislike', 'score': 0.0668652355670929},\n {'label': 'one', 'score': 0.05457024648785591}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.182315856218338},\n {'label': 'call', 'score': 0.09372124075889587},\n {'label': 'dislike', 'score': 0.0696626603603363},\n {'label': 'two_up_inverted', 'score': 0.06460952013731003},\n {'label': 'one', 'score': 0.06207621470093727}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.19654613733291626},\n {'label': 'call', 'score': 0.10379569977521896},\n {'label': 'dislike', 'score': 0.06947998702526093},\n {'label': 'two_up_inverted', 'score': 0.06915972381830215},\n {'label': 'one', 'score': 0.05416370928287506}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.1874168962240219},\n {'label': 'call', 'score': 0.10866984724998474},\n {'label': 'two_up_inverted', 'score': 0.06858141720294952},\n {'label': 'dislike', 'score': 0.06836403906345367},\n {'label': 'peace_inverted', 'score': 0.05582200735807419}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.1988355964422226},\n {'label': 'call', 'score': 0.1069992408156395},\n {'label': 'two_up_inverted', 'score': 0.0678710788488388},\n {'label': 'dislike', 'score': 0.06733830273151398},\n {'label': 'one', 'score': 0.054002195596694946}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.19083908200263977},\n {'label': 'call', 'score': 0.09821631014347076},\n {'label': 'dislike', 'score': 0.07068902254104614},\n {'label': 'two_up_inverted', 'score': 0.0686243325471878},\n {'label': 'one', 'score': 0.05701864883303642}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.19303525984287262},\n {'label': 'call', 'score': 0.09899044036865234},\n {'label': 'two_up_inverted', 'score': 0.07132618129253387},\n {'label': 'dislike', 'score': 0.0712704211473465},\n {'label': 'one', 'score': 0.05665253475308418}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.19997438788414001},\n {'label': 'call', 'score': 0.09453845024108887},\n {'label': 'two_up_inverted', 'score': 0.0721430703997612},\n {'label': 'dislike', 'score': 0.06897170096635818},\n {'label': 'one', 'score': 0.05710102245211601}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.1834455281496048},\n {'label': 'call', 'score': 0.09294099360704422},\n {'label': 'dislike', 'score': 0.07069103419780731},\n {'label': 'two_up_inverted', 'score': 0.07015776634216309},\n {'label': 'one', 'score': 0.05913538485765457}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.19791574776172638},\n {'label': 'call', 'score': 0.10942992568016052},\n {'label': 'two_up_inverted', 'score': 0.07703251391649246},\n {'label': 'dislike', 'score': 0.06650373339653015},\n {'label': 'peace_inverted', 'score': 0.053249530494213104}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.18698625266551971},\n {'label': 'call', 'score': 0.10165316611528397},\n {'label': 'two_up_inverted', 'score': 0.07433538883924484},\n {'label': 'dislike', 'score': 0.0670575499534607},\n {'label': 'one', 'score': 0.05707024037837982}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'peace_inverted', 'score': 0.15875482559204102},\n {'label': 'ok', 'score': 0.11456109583377838},\n {'label': 'three2', 'score': 0.10619701445102692},\n {'label': 'three', 'score': 0.09956875443458557},\n {'label': 'peace', 'score': 0.08840524405241013}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'peace', 'score': 0.15993638336658478},\n {'label': 'three', 'score': 0.10950279235839844},\n {'label': 'three2', 'score': 0.10365086793899536},\n {'label': 'rock', 'score': 0.10221485048532486},\n {'label': 'peace_inverted', 'score': 0.07990321516990662}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'peace', 'score': 0.23436786234378815},\n {'label': 'three', 'score': 0.11866752803325653},\n {'label': 'rock', 'score': 0.11356714367866516},\n {'label': 'three2', 'score': 0.09023352712392807},\n {'label': 'two_up', 'score': 0.06025821715593338}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'peace', 'score': 0.2234731763601303},\n {'label': 'three', 'score': 0.12090975046157837},\n {'label': 'rock', 'score': 0.1015145480632782},\n {'label': 'three2', 'score': 0.07308772951364517},\n {'label': 'two_up', 'score': 0.0640961229801178}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'peace', 'score': 0.19767922163009644},\n {'label': 'three', 'score': 0.16399893164634705},\n {'label': 'three2', 'score': 0.09100891649723053},\n {'label': 'rock', 'score': 0.0760749951004982},\n {'label': 'ok', 'score': 0.06755650043487549}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'peace', 'score': 0.22185663878917694},\n {'label': 'rock', 'score': 0.12542666494846344},\n {'label': 'three', 'score': 0.0987417995929718},\n {'label': 'three2', 'score': 0.09799948334693909},\n {'label': 'ok', 'score': 0.06360658258199692}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'peace', 'score': 0.19446729123592377},\n {'label': 'three', 'score': 0.1124214306473732},\n {'label': 'three2', 'score': 0.10896754264831543},\n {'label': 'rock', 'score': 0.1077195256948471},\n {'label': 'ok', 'score': 0.06334245949983597}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'peace', 'score': 0.26456302404403687},\n {'label': 'rock', 'score': 0.17419111728668213},\n {'label': 'three2', 'score': 0.08090173453092575},\n {'label': 'three', 'score': 0.06480627506971359},\n {'label': 'two_up', 'score': 0.058329712599515915}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'peace', 'score': 0.206048846244812},\n {'label': 'rock', 'score': 0.1869310438632965},\n {'label': 'three2', 'score': 0.10648288577795029},\n {'label': 'two_up', 'score': 0.06720878183841705},\n {'label': 'ok', 'score': 0.06663849204778671}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'three2', 'score': 0.2155197709798813},\n {'label': 'peace', 'score': 0.15594401955604553},\n {'label': 'rock', 'score': 0.11116734892129898},\n {'label': 'ok', 'score': 0.08542851358652115},\n {'label': 'three', 'score': 0.05918261408805847}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'peace', 'score': 0.22436203062534332},\n {'label': 'three2', 'score': 0.14401507377624512},\n {'label': 'rock', 'score': 0.1154932975769043},\n {'label': 'three', 'score': 0.07358888536691666},\n {'label': 'ok', 'score': 0.06386516243219376}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'peace', 'score': 0.22698351740837097},\n {'label': 'three2', 'score': 0.11261580884456635},\n {'label': 'rock', 'score': 0.09917785227298737},\n {'label': 'peace_inverted', 'score': 0.09193778783082962},\n {'label': 'three', 'score': 0.0793389156460762}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'one', 'score': 0.17618782818317413},\n {'label': 'rock', 'score': 0.11766697466373444},\n {'label': 'peace', 'score': 0.08748149126768112},\n {'label': 'mute', 'score': 0.06999506801366806},\n {'label': 'peace_inverted', 'score': 0.06266440451145172}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'three2', 'score': 0.1534537523984909},\n {'label': 'peace_inverted', 'score': 0.10584619641304016},\n {'label': 'peace', 'score': 0.10487391799688339},\n {'label': 'rock', 'score': 0.08315189927816391},\n {'label': 'three', 'score': 0.06756449490785599}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'peace', 'score': 0.2910991907119751},\n {'label': 'three2', 'score': 0.13416868448257446},\n {'label': 'rock', 'score': 0.10576700419187546},\n {'label': 'three', 'score': 0.08183376491069794},\n {'label': 'ok', 'score': 0.04726028069853783}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.14817725121974945},\n {'label': 'dislike', 'score': 0.0771327093243599},\n {'label': 'one', 'score': 0.07432103902101517},\n {'label': 'call', 'score': 0.06809070706367493},\n {'label': 'two_up_inverted', 'score': 0.06512721627950668}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.1518324315547943},\n {'label': 'call', 'score': 0.08298564702272415},\n {'label': 'one', 'score': 0.07055182009935379},\n {'label': 'two_up_inverted', 'score': 0.06904960423707962},\n {'label': 'dislike', 'score': 0.0676414743065834}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.18444563448429108},\n {'label': 'call', 'score': 0.10045512020587921},\n {'label': 'two_up_inverted', 'score': 0.07001128047704697},\n {'label': 'dislike', 'score': 0.06876858323812485},\n {'label': 'one', 'score': 0.05672364681959152}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.19349487125873566},\n {'label': 'call', 'score': 0.11339413374662399},\n {'label': 'two_up_inverted', 'score': 0.0738324522972107},\n {'label': 'dislike', 'score': 0.06338012963533401},\n {'label': 'one', 'score': 0.05467534437775612}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.19386474788188934},\n {'label': 'call', 'score': 0.1109779104590416},\n {'label': 'two_up_inverted', 'score': 0.07111084461212158},\n {'label': 'dislike', 'score': 0.06769249588251114},\n {'label': 'one', 'score': 0.05320147052407265}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.21081684529781342},\n {'label': 'call', 'score': 0.1002652570605278},\n {'label': 'dislike', 'score': 0.07045291364192963},\n {'label': 'two_up_inverted', 'score': 0.06752762198448181},\n {'label': 'fist', 'score': 0.052240192890167236}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.22896580398082733},\n {'label': 'call', 'score': 0.09278856962919235},\n {'label': 'two_up_inverted', 'score': 0.07120370119810104},\n {'label': 'dislike', 'score': 0.0703597217798233},\n {'label': 'one', 'score': 0.051108114421367645}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.22521010041236877},\n {'label': 'call', 'score': 0.09063766151666641},\n {'label': 'dislike', 'score': 0.07195638120174408},\n {'label': 'two_up_inverted', 'score': 0.07024870067834854},\n {'label': 'mute', 'score': 0.05507678911089897}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.21538081765174866},\n {'label': 'call', 'score': 0.08427869528532028},\n {'label': 'two_up_inverted', 'score': 0.07174965739250183},\n {'label': 'dislike', 'score': 0.07099068909883499},\n {'label': 'one', 'score': 0.056071460247039795}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.22454816102981567},\n {'label': 'call', 'score': 0.09693165123462677},\n {'label': 'two_up_inverted', 'score': 0.07129193842411041},\n {'label': 'dislike', 'score': 0.07071582973003387},\n {'label': 'one', 'score': 0.053903527557849884}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.22272519767284393},\n {'label': 'call', 'score': 0.12747406959533691},\n {'label': 'dislike', 'score': 0.07062450051307678},\n {'label': 'two_up_inverted', 'score': 0.06561654061079025},\n {'label': 'mute', 'score': 0.05733952298760414}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.21034552156925201},\n {'label': 'call', 'score': 0.1147666871547699},\n {'label': 'dislike', 'score': 0.06652914732694626},\n {'label': 'two_up_inverted', 'score': 0.06603112071752548},\n {'label': 'mute', 'score': 0.05606750026345253}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.21298055350780487},\n {'label': 'call', 'score': 0.09883390367031097},\n {'label': 'dislike', 'score': 0.06801360100507736},\n {'label': 'two_up_inverted', 'score': 0.06348909437656403},\n {'label': 'one', 'score': 0.055527374148368835}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.22592134773731232},\n {'label': 'call', 'score': 0.10717575997114182},\n {'label': 'dislike', 'score': 0.06995332986116409},\n {'label': 'two_up_inverted', 'score': 0.06809979677200317},\n {'label': 'mute', 'score': 0.05474812537431717}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.2183588296175003},\n {'label': 'call', 'score': 0.1216893270611763},\n {'label': 'dislike', 'score': 0.0687115341424942},\n {'label': 'two_up_inverted', 'score': 0.06738694757223129},\n {'label': 'mute', 'score': 0.06732850521802902}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.21757623553276062},\n {'label': 'call', 'score': 0.11518115550279617},\n {'label': 'dislike', 'score': 0.07228043675422668},\n {'label': 'two_up_inverted', 'score': 0.06785629689693451},\n {'label': 'mute', 'score': 0.06072824448347092}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.22621741890907288},\n {'label': 'call', 'score': 0.10414239764213562},\n {'label': 'dislike', 'score': 0.07320764660835266},\n {'label': 'two_up_inverted', 'score': 0.06731370091438293},\n {'label': 'mute', 'score': 0.05562039464712143}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.21280144155025482},\n {'label': 'call', 'score': 0.1149178072810173},\n {'label': 'dislike', 'score': 0.07021413743495941},\n {'label': 'two_up_inverted', 'score': 0.06666700541973114},\n {'label': 'mute', 'score': 0.06429354101419449}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.2180209904909134},\n {'label': 'call', 'score': 0.10222899168729782},\n {'label': 'dislike', 'score': 0.0702052116394043},\n {'label': 'two_up_inverted', 'score': 0.06740593910217285},\n {'label': 'mute', 'score': 0.0544692799448967}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.2024613320827484},\n {'label': 'call', 'score': 0.11858458071947098},\n {'label': 'dislike', 'score': 0.06880838423967361},\n {'label': 'two_up_inverted', 'score': 0.0640171468257904},\n {'label': 'mute', 'score': 0.05864792689681053}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.2108614146709442},\n {'label': 'call', 'score': 0.13325315713882446},\n {'label': 'dislike', 'score': 0.06881748884916306},\n {'label': 'two_up_inverted', 'score': 0.06356405466794968},\n {'label': 'mute', 'score': 0.059434857219457626}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.21404844522476196},\n {'label': 'call', 'score': 0.11986897885799408},\n {'label': 'dislike', 'score': 0.06823081523180008},\n {'label': 'two_up_inverted', 'score': 0.06611767411231995},\n {'label': 'mute', 'score': 0.06013120338320732}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.19158940017223358},\n {'label': 'call', 'score': 0.10449755191802979},\n {'label': 'dislike', 'score': 0.07184363156557083},\n {'label': 'two_up_inverted', 'score': 0.069517582654953},\n {'label': 'one', 'score': 0.06394001096487045}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.17731498181819916},\n {'label': 'call', 'score': 0.09388431906700134},\n {'label': 'one', 'score': 0.07831215858459473},\n {'label': 'dislike', 'score': 0.07523908466100693},\n {'label': 'two_up_inverted', 'score': 0.06680792570114136}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.2005237489938736},\n {'label': 'call', 'score': 0.1256713569164276},\n {'label': 'mute', 'score': 0.0699470266699791},\n {'label': 'dislike', 'score': 0.06653834134340286},\n {'label': 'one', 'score': 0.0635763481259346}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.18989422917366028},\n {'label': 'call', 'score': 0.09309019148349762},\n {'label': 'dislike', 'score': 0.07465384900569916},\n {'label': 'one', 'score': 0.07110358774662018},\n {'label': 'two_up_inverted', 'score': 0.0676121637225151}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.18414144217967987},\n {'label': 'one', 'score': 0.08131363987922668},\n {'label': 'dislike', 'score': 0.07967908680438995},\n {'label': 'call', 'score': 0.07753148674964905},\n {'label': 'two_up_inverted', 'score': 0.0636775940656662}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.1963297724723816},\n {'label': 'dislike', 'score': 0.08110172301530838},\n {'label': 'call', 'score': 0.07969889044761658},\n {'label': 'one', 'score': 0.07399529218673706},\n {'label': 'two_up_inverted', 'score': 0.06280755996704102}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.16878169775009155},\n {'label': 'one', 'score': 0.08978220075368881},\n {'label': 'dislike', 'score': 0.08388537913560867},\n {'label': 'call', 'score': 0.07677887380123138},\n {'label': 'mute', 'score': 0.0666043758392334}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.16569112241268158},\n {'label': 'one', 'score': 0.08670753240585327},\n {'label': 'dislike', 'score': 0.08007866889238358},\n {'label': 'call', 'score': 0.07404632121324539},\n {'label': 'two_up_inverted', 'score': 0.06140102818608284}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.14465321600437164},\n {'label': 'one', 'score': 0.09402094781398773},\n {'label': 'dislike', 'score': 0.07989805936813354},\n {'label': 'call', 'score': 0.06678437441587448},\n {'label': 'fist', 'score': 0.06006518378853798}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.14245392382144928},\n {'label': 'one', 'score': 0.09133449196815491},\n {'label': 'dislike', 'score': 0.08449754863977432},\n {'label': 'call', 'score': 0.07607581466436386},\n {'label': 'fist', 'score': 0.06175176426768303}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.14386394619941711},\n {'label': 'one', 'score': 0.09380608797073364},\n {'label': 'dislike', 'score': 0.07734353840351105},\n {'label': 'call', 'score': 0.07241320610046387},\n {'label': 'fist', 'score': 0.06208564341068268}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.1276378333568573},\n {'label': 'one', 'score': 0.10174731910228729},\n {'label': 'dislike', 'score': 0.07924488186836243},\n {'label': 'fist', 'score': 0.06409464031457901},\n {'label': 'call', 'score': 0.06308726221323013}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.12029938399791718},\n {'label': 'one', 'score': 0.10369373112916946},\n {'label': 'dislike', 'score': 0.08214014023542404},\n {'label': 'fist', 'score': 0.06505865603685379},\n {'label': 'call', 'score': 0.062040720134973526}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.11873830854892731},\n {'label': 'one', 'score': 0.09859896451234818},\n {'label': 'dislike', 'score': 0.0813315287232399},\n {'label': 'fist', 'score': 0.06860716640949249},\n {'label': 'call', 'score': 0.06367217004299164}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.14360827207565308},\n {'label': 'one', 'score': 0.08538467437028885},\n {'label': 'dislike', 'score': 0.08200541883707047},\n {'label': 'call', 'score': 0.07399939745664597},\n {'label': 'fist', 'score': 0.06461597234010696}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.125327467918396},\n {'label': 'one', 'score': 0.09455988556146622},\n {'label': 'dislike', 'score': 0.07888312637805939},\n {'label': 'fist', 'score': 0.07274915277957916},\n {'label': 'call', 'score': 0.06370618939399719}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.10821574181318283},\n {'label': 'one', 'score': 0.10481761395931244},\n {'label': 'dislike', 'score': 0.08495010435581207},\n {'label': 'fist', 'score': 0.07137630879878998},\n {'label': 'call', 'score': 0.06036071479320526}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.13548292219638824},\n {'label': 'two_up_inverted', 'score': 0.08761032670736313},\n {'label': 'call', 'score': 0.08052082359790802},\n {'label': 'dislike', 'score': 0.08046942949295044},\n {'label': 'fist', 'score': 0.07025512307882309}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'two_up_inverted', 'score': 0.2779674232006073},\n {'label': 'peace_inverted', 'score': 0.06921440362930298},\n {'label': 'stop_inverted', 'score': 0.06458595395088196},\n {'label': 'like', 'score': 0.06319660693407059},\n {'label': 'fist', 'score': 0.05959625542163849}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.21326524019241333},\n {'label': 'call', 'score': 0.1564035266637802},\n {'label': 'mute', 'score': 0.07899170368909836},\n {'label': 'dislike', 'score': 0.07197067886590958},\n {'label': 'two_up_inverted', 'score': 0.06409517675638199}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.19598743319511414},\n {'label': 'call', 'score': 0.14561375975608826},\n {'label': 'two_up_inverted', 'score': 0.07261134684085846},\n {'label': 'dislike', 'score': 0.06842902302742004},\n {'label': 'mute', 'score': 0.06401871889829636}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.213914692401886},\n {'label': 'call', 'score': 0.11571113020181656},\n {'label': 'two_up_inverted', 'score': 0.07453524321317673},\n {'label': 'dislike', 'score': 0.07384428381919861},\n {'label': 'mute', 'score': 0.06176326051354408}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.20392340421676636},\n {'label': 'call', 'score': 0.13000497221946716},\n {'label': 'dislike', 'score': 0.08088324218988419},\n {'label': 'mute', 'score': 0.07140544801950455},\n {'label': 'two_up_inverted', 'score': 0.0682932659983635}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.2324667125940323},\n {'label': 'call', 'score': 0.13631491363048553},\n {'label': 'mute', 'score': 0.06889382004737854},\n {'label': 'dislike', 'score': 0.06889038532972336},\n {'label': 'two_up_inverted', 'score': 0.06814039498567581}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.2509540915489197},\n {'label': 'call', 'score': 0.13009236752986908},\n {'label': 'two_up_inverted', 'score': 0.06847751140594482},\n {'label': 'dislike', 'score': 0.06615090370178223},\n {'label': 'mute', 'score': 0.06468627601861954}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[{'label': 'like', 'score': 0.23753955960273743},\n {'label': 'call', 'score': 0.15695925056934357},\n {'label': 'two_up_inverted', 'score': 0.06963308155536652},\n {'label': 'dislike', 'score': 0.06692629307508469},\n {'label': 'mute', 'score': 0.06072952598333359}]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[48], line 21\u001B[0m\n\u001B[0;32m     19\u001B[0m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m     20\u001B[0m         cap\u001B[38;5;241m.\u001B[39mrelease()\n\u001B[1;32m---> 21\u001B[0m \u001B[43mwebcam_to_pipe\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[48], line 17\u001B[0m, in \u001B[0;36mwebcam_to_pipe\u001B[1;34m()\u001B[0m\n\u001B[0;32m     15\u001B[0m         frame_base64 \u001B[38;5;241m=\u001B[39m base64\u001B[38;5;241m.\u001B[39mb64encode(buffer)\u001B[38;5;241m.\u001B[39mdecode(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mutf-8\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m     16\u001B[0m         \u001B[38;5;28;01mwith\u001B[39;00m BytesIO(buffer) \u001B[38;5;28;01mas\u001B[39;00m img_stream:\n\u001B[1;32m---> 17\u001B[0m             img_pipe \u001B[38;5;241m=\u001B[39m \u001B[43mpipe\u001B[49m\u001B[43m(\u001B[49m\u001B[43mframe_base64\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     18\u001B[0m             display((img_pipe))\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
      "File \u001B[1;32mc:\\users\\brave\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\pipelines\\image_classification.py:157\u001B[0m, in \u001B[0;36mImageClassificationPipeline.__call__\u001B[1;34m(self, images, **kwargs)\u001B[0m\n\u001B[0;32m    110\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, images: Union[\u001B[38;5;28mstr\u001B[39m, List[\u001B[38;5;28mstr\u001B[39m], \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mImage.Image\u001B[39m\u001B[38;5;124m\"\u001B[39m, List[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mImage.Image\u001B[39m\u001B[38;5;124m\"\u001B[39m]], \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m    111\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    112\u001B[0m \u001B[38;5;124;03m    Assign labels to the image(s) passed as inputs.\u001B[39;00m\n\u001B[0;32m    113\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    155\u001B[0m \u001B[38;5;124;03m        - **score** (`int`) -- The score attributed by the model for that label.\u001B[39;00m\n\u001B[0;32m    156\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 157\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m(images, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mc:\\users\\brave\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\pipelines\\base.py:1196\u001B[0m, in \u001B[0;36mPipeline.__call__\u001B[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1188\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mnext\u001B[39m(\n\u001B[0;32m   1189\u001B[0m         \u001B[38;5;28miter\u001B[39m(\n\u001B[0;32m   1190\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_iterator(\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1193\u001B[0m         )\n\u001B[0;32m   1194\u001B[0m     )\n\u001B[0;32m   1195\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1196\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_single\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpreprocess_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mforward_params\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpostprocess_params\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mc:\\users\\brave\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\pipelines\\base.py:1203\u001B[0m, in \u001B[0;36mPipeline.run_single\u001B[1;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001B[0m\n\u001B[0;32m   1201\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrun_single\u001B[39m(\u001B[38;5;28mself\u001B[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001B[0;32m   1202\u001B[0m     model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocess(inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpreprocess_params)\n\u001B[1;32m-> 1203\u001B[0m     model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward(model_inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mforward_params)\n\u001B[0;32m   1204\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpostprocess(model_outputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mpostprocess_params)\n\u001B[0;32m   1205\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[1;32mc:\\users\\brave\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\pipelines\\base.py:1102\u001B[0m, in \u001B[0;36mPipeline.forward\u001B[1;34m(self, model_inputs, **forward_params)\u001B[0m\n\u001B[0;32m   1100\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m inference_context():\n\u001B[0;32m   1101\u001B[0m         model_inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_inputs, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m-> 1102\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward(model_inputs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mforward_params)\n\u001B[0;32m   1103\u001B[0m         model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ensure_tensor_on_device(model_outputs, device\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m   1104\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[1;32mc:\\users\\brave\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\pipelines\\image_classification.py:165\u001B[0m, in \u001B[0;36mImageClassificationPipeline._forward\u001B[1;34m(self, model_inputs)\u001B[0m\n\u001B[0;32m    164\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_forward\u001B[39m(\u001B[38;5;28mself\u001B[39m, model_inputs):\n\u001B[1;32m--> 165\u001B[0m     model_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mmodel_inputs)\n\u001B[0;32m    166\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m model_outputs\n",
      "File \u001B[1;32mc:\\users\\brave\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mc:\\users\\brave\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mc:\\users\\brave\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:794\u001B[0m, in \u001B[0;36mViTForImageClassification.forward\u001B[1;34m(self, pixel_values, head_mask, labels, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001B[0m\n\u001B[0;32m    786\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    787\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001B[39;00m\n\u001B[0;32m    788\u001B[0m \u001B[38;5;124;03m    Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\u001B[39;00m\n\u001B[0;32m    789\u001B[0m \u001B[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001B[39;00m\n\u001B[0;32m    790\u001B[0m \u001B[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[0;32m    791\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    792\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[1;32m--> 794\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mvit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    795\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpixel_values\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    796\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    797\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    798\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    799\u001B[0m \u001B[43m    \u001B[49m\u001B[43minterpolate_pos_encoding\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minterpolate_pos_encoding\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    800\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    801\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    803\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    805\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mclassifier(sequence_output[:, \u001B[38;5;241m0\u001B[39m, :])\n",
      "File \u001B[1;32mc:\\users\\brave\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mc:\\users\\brave\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mc:\\users\\brave\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:577\u001B[0m, in \u001B[0;36mViTModel.forward\u001B[1;34m(self, pixel_values, bool_masked_pos, head_mask, output_attentions, output_hidden_states, interpolate_pos_encoding, return_dict)\u001B[0m\n\u001B[0;32m    571\u001B[0m     pixel_values \u001B[38;5;241m=\u001B[39m pixel_values\u001B[38;5;241m.\u001B[39mto(expected_dtype)\n\u001B[0;32m    573\u001B[0m embedding_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(\n\u001B[0;32m    574\u001B[0m     pixel_values, bool_masked_pos\u001B[38;5;241m=\u001B[39mbool_masked_pos, interpolate_pos_encoding\u001B[38;5;241m=\u001B[39minterpolate_pos_encoding\n\u001B[0;32m    575\u001B[0m )\n\u001B[1;32m--> 577\u001B[0m encoder_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    578\u001B[0m \u001B[43m    \u001B[49m\u001B[43membedding_output\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    579\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    580\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    581\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    582\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    583\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    584\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m encoder_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    585\u001B[0m sequence_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayernorm(sequence_output)\n",
      "File \u001B[1;32mc:\\users\\brave\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mc:\\users\\brave\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mc:\\users\\brave\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:407\u001B[0m, in \u001B[0;36mViTEncoder.forward\u001B[1;34m(self, hidden_states, head_mask, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[0;32m    400\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gradient_checkpointing_func(\n\u001B[0;32m    401\u001B[0m         layer_module\u001B[38;5;241m.\u001B[39m\u001B[38;5;21m__call__\u001B[39m,\n\u001B[0;32m    402\u001B[0m         hidden_states,\n\u001B[0;32m    403\u001B[0m         layer_head_mask,\n\u001B[0;32m    404\u001B[0m         output_attentions,\n\u001B[0;32m    405\u001B[0m     )\n\u001B[0;32m    406\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 407\u001B[0m     layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlayer_head_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    409\u001B[0m hidden_states \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m0\u001B[39m]\n\u001B[0;32m    411\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_attentions:\n",
      "File \u001B[1;32mc:\\users\\brave\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mc:\\users\\brave\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mc:\\users\\brave\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:368\u001B[0m, in \u001B[0;36mViTLayer.forward\u001B[1;34m(self, hidden_states, head_mask, output_attentions)\u001B[0m\n\u001B[0;32m    365\u001B[0m layer_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mintermediate(layer_output)\n\u001B[0;32m    367\u001B[0m \u001B[38;5;66;03m# second residual connection is done here\u001B[39;00m\n\u001B[1;32m--> 368\u001B[0m layer_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43moutput\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlayer_output\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    370\u001B[0m outputs \u001B[38;5;241m=\u001B[39m (layer_output,) \u001B[38;5;241m+\u001B[39m outputs\n\u001B[0;32m    372\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m outputs\n",
      "File \u001B[1;32mc:\\users\\brave\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mc:\\users\\brave\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mc:\\users\\brave\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\transformers\\models\\vit\\modeling_vit.py:325\u001B[0m, in \u001B[0;36mViTOutput.forward\u001B[1;34m(self, hidden_states, input_tensor)\u001B[0m\n\u001B[0;32m    324\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, hidden_states: torch\u001B[38;5;241m.\u001B[39mTensor, input_tensor: torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[1;32m--> 325\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdense\u001B[49m\u001B[43m(\u001B[49m\u001B[43mhidden_states\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    326\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(hidden_states)\n\u001B[0;32m    328\u001B[0m     hidden_states \u001B[38;5;241m=\u001B[39m hidden_states \u001B[38;5;241m+\u001B[39m input_tensor\n",
      "File \u001B[1;32mc:\\users\\brave\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1509\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1510\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1511\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mc:\\users\\brave\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1515\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1516\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1518\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1519\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1520\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1523\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mc:\\users\\brave\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    115\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 116\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def webcam_to_pipe():\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            ret, frame = cap.read()\n",
    "            \n",
    "            if not ret:\n",
    "                print(\"Error reading frame from the webcam.\")\n",
    "                break\n",
    "            \n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            _, buffer = cv2.imencode('.jpg', frame_rgb)\n",
    "            frame_base64 = base64.b64encode(buffer).decode('utf-8')\n",
    "            with BytesIO(buffer) as img_stream:\n",
    "                img_pipe = pipe(frame_base64)\n",
    "                display((img_pipe))\n",
    "    finally:\n",
    "        cap.release()\n",
    "webcam_to_pipe()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-05T07:12:48.795546400Z",
     "start_time": "2024-03-05T07:12:07.556108900Z"
    }
   },
   "id": "d31d4955f97d69c0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a6762c672b4218bc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
